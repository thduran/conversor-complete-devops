[Para PT-BR, clique aqui](#conversor-api---devops-end-to-end-portf√≥lio)

# Converter - DevOps End-to-End (Portfolio)

## 1. About the project


---

PT-BR:

# Conversor - DevOps End-to-End (Portf√≥lio)

## 1. Sobre o projeto

Este projeto √© um laborat√≥rio pr√°tico de DevOps, simulando um ciclo de vida real de software, cobrindo:

- Containeriza√ß√£o (Docker)
- Deploy em Kubernetes (staging e produ√ß√£o)
- Escalabilidade (HPA) baseada em CPU e mem√≥ria
- CI/CD com GitHub Actions
- Monitoramento com Prometheus e Grafana
- Alertmanager integrado ao Slack
- Testes automatizados

‚ö†Ô∏è O objetivo n√£o √© a aplica√ß√£o em si, mas demonstrar a aplica√ß√£o de pr√°ticas DevOps completo do zero at√© produ√ß√£o.

## 2. O que o projeto faz

- API simples de convers√£o de moeda (`/converter`)
- Salva hist√≥rico de convers√µes (`/history`) em PostgreSQL
- Exp√µe m√©tricas via Prometheus (`/metrics`)  
- Automatiza build, teste e deploy com GitHub Actions
- Permite observabilidade via Grafana e alertas no Slack

## 3. Como rodar o projeto

### 3.1 Pr√©-requisitos

- Cluster Kubernetes (DigitalOcean, minikube, kind)
- `kubectl` configurado  
- Docker instalado 
- Conta no Slack (ou outro) pra alertas

### 3.2 Clone o reposit√≥rio
```bash
git clone https://github.com/thduran/conversor-complete-devops.git
cd conversor-complete-devops
```

### 3.3 Configurar secrets

1. Criar namespaces
```bash
kubectl apply -f k8s/base
```

#### Banco de dados

Crie a secret pro Postgres:

```bash
# Staging
kubectl create secret generic db-admin-pass \
  --from-literal=PASSWORD="my-password" \
  -n staging

# Production
kubectl create secret generic db-admin-pass \
  --from-literal=PASSWORD="my-password" \
  -n production
```

Crie a secret para a aplica√ß√£o:

```bash
# Staging
kubectl create secret generic db-credentials \
  --from-literal=DATABASE_URL="postgresql://postgres:my-password@postgres-svc:5432/app_db" \
  -n staging

# Production
kubectl create secret generic db-credentials \
  --from-literal=DATABASE_URL="postgresql://postgres:my-password@postgres-svc:5432/app_db" \
  -n production
```

#### Grafana (opcional)
```bash
export GRAFANA_PASSWORD="my-password"
kubectl create secret generic grafana-admin \
  --from-literal=password="$GRAFANA_PASSWORD" \
  -n monitoring
```

### 3.4 Deploy manual (sem CI/CD)

1. Deploy do banco, API, Prometheus e Grafana:
```bash
kubectl apply -f k8s/base/staging
kubectl apply -f k8s/base/production
kubectl apply -f k8s/prometheus -R
kubectl apply -f k8s/grafana
```

2. Verificar pods e servi√ßos:
```bash
kubectl get pods -n staging
kubectl get pods -n production
kubectl get svc -n staging
kubectl get svc -n production
```

3. Teste com IPs do LoadBalancer

http://<IP_STAGING>/converter?value=10&from=usd&to=brl \
http://<IP_PRODUCTION>/converter?value=20&from=usd&to=brl \
http://<IP_PRODUCTION>/history 

Mas se estiver rodando localmente, execute: \
`kubectl port-forward -n production svc/conversor 8080:80` \
`kubectl port-forward -n monitoring svc/prometheus 9090:9090` \
`kubectl port-forward -n monitoring svc/grafana 3000:3000`

## 4. Validando elasticidade (teste HPA)

### 4.1 Abra o monitoramento do HPA
`kubectl get hpa -n production -w`

### 4.2 Gere carga nos 2 pods existentes
`kubectl exec -it conversor-PODNAME -n production -- python -c "while True: 10**1000"`

No Grafana, voc√™ ver√° que o gr√°fico _CPU PER PODS_ vai subir e, como o HPA vai detectar o uso, voc√™ poder√° ver o n√∫mero de r√©plicas subindo tanto no terminal como no gr√°fico _NUMBER OF PODS_. _MEMORY_ vai ficar est√°vel pois apenas CPU √© requisitado.

## 5. Desafios e aprendizados

- Quais n√∫meros colocar no HPA -> o Grafana foi utilizado para ajustar os par√¢metros do HPA conforme √† realidade da aplica√ß√£o.
- Prometheus n√£o enxerga novos pods -> implementa√ß√£o do Service Discovery para que novos pods sejam vistos automaticamente.
- Github bloqueou push de secrets -> recriar arquivos com placeholders √© uma boa pr√°tica
- Push no Docker Hub negado (insufficient scopes) -> corrigido criando token com escopo de escrita.
- Github n√£o encontra path -> n√£o havia feito push a partir da raiz
- Deploy sem testes nem aprova√ß√£o -> validar primeiro o CI antes de expandir para CD.
- Workflow falhava no job de produ√ß√£o (kubectl conectava em localhost:8080) -> definir vari√°vel KUBECONFIG em todos os jobs necess√°rios
- Tornar o projeto port√°vel -> substituir todos os valores reais de senhas, IPs, tokens por placeholders ou secrets, al√©m de documentar o uso no README de forma clara.
- Gunicorn n√£o encontrado -> depend√™ncias precisam estar nos requirements.txt para serem "vistas"

## 6. Demonstra√ß√£o visual

üìÑ [CLIQUE AQUI para ver as imagens: p.1 - registro de convers√µes no banco; p.2- Alerta no Slack; p.3 - Alerta no Prometheus; p.4 - Jobs do Actions; p.5 - Gr√°ficos no Grafana](docs/img.pdf)

Na p.5, perceba o HPA atuando: aumentando ou diminuindo o n√∫mero de pods conforme o necess√°rio.

### 7. CI/CD

O workflow (.github/workflows/ci-cd.yaml) √© executado a cada push na branch main e realiza:
- Build e push da imagem Docker para o Docker Hub
- Deploy autom√°tico em ambiente Kubernetes de staging
- Deploy em produ√ß√£o (Digital Ocean) requer aprova√ß√£o manual
- Testes automatizados em Python
 

### ‚úÖ 8. Conclus√£o

Este projeto consolidou conhecimentos fundamentais para a atua√ß√£o como DevOps Engineer, demonstrando a capacidade de arquitetar, automatizar e monitorar aplica√ß√µes modernas em ambientes containerizados.